Ollama Modelfiles and Benchmark\n\nBuild models with larger default context and run a simple latency benchmark.\n\nBuild models:\n\n```powershell\ncd tools/ollama\nollama create qwen3-coder-30b-ctx128k -f ./Modelfile.qwen3-coder-30b-ctx128k\nollama create qwen3-30b-ctx128k -f ./Modelfile.qwen3-30b-ctx128k\nollama create gpt-oss-latest-ctx128k -f ./Modelfile.gpt-oss-latest-ctx128k\n```\n\nRun benchmark (start `ollama serve` first):\n\n```powershell\n# In another terminal\nollama serve --num-ctx 128000\n\n# Benchmark each model\n./benchmark.ps1 -Model qwen3-coder-30b-ctx128k -NumCtx 128000 -Rounds 3\n./benchmark.ps1 -Model qwen3-30b-ctx128k -NumCtx 128000 -Rounds 3\n./benchmark.ps1 -Model gpt-oss-latest-ctx128k -NumCtx 128000 -Rounds 3\n```\n\nNotes:\n- Increase `--num-ctx` when serving if you need longer windows (up to the model's max).\n- If you hit OOM, reduce `num_ctx` or use lighter quantization.\n- Adjust temperature/top_p in Modelfiles for your style.\n